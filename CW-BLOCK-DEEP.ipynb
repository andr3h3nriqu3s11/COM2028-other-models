{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, utils\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, to_categorical\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = ''# kaggle username\n",
    "os.environ['KAGGLE_KEY'] = ''# kaggle key\n",
    "os.environ['URN'] = '6644818'  # Your URN: submissions without a URN will not count#\n",
    "\n",
    "!kaggle competitions download -c uos-com2028-21-22-cw()\n",
    "!unzip uos-com2028-21-22-cw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 11:18:20.727853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:20.775866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:20.776053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Configure tensorflow to use as much memory as possible\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mode = 'grayscale'\n",
    "image_size = (32, 32)\n",
    "image_shape = (*image_size, 1)\n",
    "batch_size = 800\n",
    "\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 11:18:20.813852: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 11:18:20.814761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:20.814919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:20.815037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:21.550133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:21.550273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:21.550384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 11:18:21.550669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4929 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# read all labels\n",
    "train_labels_dp = pd.read_csv('train.csv')\n",
    "train_labels = tf.constant(train_labels_dp.loc[:, 'Cell type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.tensorflow.org/tutorials/load_data/images\n",
    "# Helper functions to help load the data into the datasets\n",
    "\n",
    "def pathToLabel(path):\n",
    "  path = tf.strings.regex_replace(path, \"./train/\", \"\")\n",
    "  path = tf.strings.regex_replace(path, \".jpg\", \"\")\n",
    "  return train_labels[tf.strings.to_number(path, out_type=tf.int32)]\n",
    "\n",
    "def decode_image(img):\n",
    "  # channels were reduced to 1 since image is grayscale\n",
    "  img = tf.io.decode_jpeg(img, channels=1)\n",
    "\n",
    "  return tf.image.resize(img, image_size)\n",
    "\n",
    "def process_path(path, addPath=False):\n",
    "  label = pathToLabel(path)\n",
    "\n",
    "  img = tf.io.read_file(path)\n",
    "  img = decode_image(img)\n",
    "\n",
    "  if addPath:\n",
    "    return img, label, path\n",
    "  else:\n",
    "    return img, label\n",
    "\n",
    "def configure_for_performance(ds: tf.data.Dataset) -> tf.data.Dataset:\n",
    "  #ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size= 1000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def prepare_dataset(ds: tf.data.Dataset) -> tf.data.Dataset:\n",
    "  ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "  ds = configure_for_performance(ds)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 2403\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 10000)\n",
    "\n",
    "print(\"seed: %d\" % seed)\n",
    "\n",
    "# Read all the files from the direcotry\n",
    "list_ds = tf.data.Dataset.list_files(str('./train/*'), shuffle=False)\n",
    "\n",
    "image_count = len(list_ds)\n",
    "\n",
    "list_ds = list_ds.shuffle(image_count, seed=seed)\n",
    "\n",
    "val_size = int(image_count * 0.333)\n",
    "\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n",
    "\n",
    "train_ds = prepare_dataset(train_ds)\n",
    "val_ds = prepare_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to show some images\n",
    "\n",
    "# image_batch, label_batch  = next(iter(train_ds))\n",
    "\n",
    "# plt.figure(figsize=image_size)\n",
    "# for i in range(4):\n",
    "#   ax = plt.subplot(2, 2, i + 1)\n",
    "#   plt.imshow(image_batch[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "\n",
    "#   label = label_batch[i]\n",
    "#   path = \"\"#str(path_batch[i].numpy())\n",
    "\n",
    "#   plt.title(str(label.numpy()) + \" \" + path)\n",
    "#   plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " 0-1-64-1 (Sequential)       (None, 32, 32, 64)        384       \n",
      "                                                                 \n",
      " 1-3-128-3 (Sequential)      (None, 16, 16, 128)       369536    \n",
      "                                                                 \n",
      " 2-4-250-3 (Sequential)      (None, 8, 8, 250)         1977500   \n",
      "                                                                 \n",
      " 3-5-500-3 (Sequential)      (None, 4, 4, 500)         10129500  \n",
      "                                                                 \n",
      " 4-1-1000-3 (Sequential)     (None, 4, 4, 1000)        4505000   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1000)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 8008      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,989,928\n",
      "Trainable params: 16,986,044\n",
      "Non-trainable params: 3,884\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=image_shape),\n",
    "    layers.RandomRotation(0.4),\n",
    "    layers.RandomContrast(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "track = 0\n",
    "\n",
    "\n",
    "def addBlock(\n",
    "    b_size: int,\n",
    "    filter_size: int,\n",
    "    kernel_size: int = 3,\n",
    "    top: bool = True,\n",
    "    pooling_same: bool = False,\n",
    "    pool_func=layers.MaxPool2D\n",
    "):\n",
    "    global track\n",
    "    model = keras.Sequential(name=f\"{track}-{b_size}-{filter_size}-{kernel_size}\")\n",
    "    track += 1\n",
    "    for _ in range(b_size):\n",
    "        model.add(layers.Conv2D(\n",
    "            filter_size,\n",
    "            kernel_size,\n",
    "            padding=\"same\"\n",
    "        ))\n",
    "        model.add(layers.ReLU())\n",
    "    if top:\n",
    "        if pooling_same:\n",
    "            model.add(pool_func(padding=\"same\", strides=(1, 1)))\n",
    "        else:\n",
    "            model.add(pool_func())\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(data_augmentation)\n",
    "\n",
    "model.add(layers.Rescaling(1.0/255, input_shape=image_shape))\n",
    "\n",
    "model.add(addBlock(1, 64, 1, pooling_same=True, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(3, 128, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(4, 250, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(5, 500, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(1, 1000, 3, pooling_same=True, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(layers.GlobalAvgPool2D())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Softmax())\n",
    "\n",
    "model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  optimizer=tfa.optimizers.AdamW(weight_decay=1e-4),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/keras/regression\n",
    "def plot_loss(history, val=True, color=\"b\", save=False):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    if val:\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    if save:\n",
    "        plt.savefig('./fig.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 00:27:49.362106: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400\n",
      "2022-05-10 00:27:50.668072: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:50.668103: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:50.668112: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:50.668119: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.154903: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.154933: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.154942: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.75GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.154949: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.75GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.227695: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-10 00:27:51.227720: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/126 [============================>.] - ETA: 1s - loss: 1.7791 - accuracy: 0.3837"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 00:30:09.891887: W tensorflow/core/common_runtime/bfc_allocator.cc:343] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - ETA: 0s - loss: 1.7790 - accuracy: 0.3837\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28607, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 170s 1s/step - loss: 1.7790 - accuracy: 0.3837 - val_loss: 1.9562 - val_accuracy: 0.2861\n",
      "Epoch 2/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.4386 - accuracy: 0.4890\n",
      "Epoch 2: val_accuracy did not improve from 0.28607\n",
      "126/126 [==============================] - 145s 1s/step - loss: 1.4386 - accuracy: 0.4890 - val_loss: 6.2393 - val_accuracy: 0.2351\n",
      "Epoch 3/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.3427 - accuracy: 0.5118\n",
      "Epoch 3: val_accuracy did not improve from 0.28607\n",
      "126/126 [==============================] - 147s 1s/step - loss: 1.3427 - accuracy: 0.5118 - val_loss: 4.9871 - val_accuracy: 0.2295\n",
      "Epoch 4/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.2848 - accuracy: 0.5322\n",
      "Epoch 4: val_accuracy did not improve from 0.28607\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.2848 - accuracy: 0.5322 - val_loss: 4.0407 - val_accuracy: 0.2444\n",
      "Epoch 5/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.2531 - accuracy: 0.5448\n",
      "Epoch 5: val_accuracy improved from 0.28607 to 0.51588, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.2531 - accuracy: 0.5448 - val_loss: 1.3332 - val_accuracy: 0.5159\n",
      "Epoch 6/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.2251 - accuracy: 0.5546\n",
      "Epoch 6: val_accuracy improved from 0.51588 to 0.56965, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 147s 1s/step - loss: 1.2251 - accuracy: 0.5546 - val_loss: 1.1896 - val_accuracy: 0.5696\n",
      "Epoch 7/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.2023 - accuracy: 0.5620\n",
      "Epoch 7: val_accuracy did not improve from 0.56965\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.2023 - accuracy: 0.5620 - val_loss: 1.3538 - val_accuracy: 0.5408\n",
      "Epoch 8/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1849 - accuracy: 0.5689\n",
      "Epoch 8: val_accuracy did not improve from 0.56965\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.1849 - accuracy: 0.5689 - val_loss: 1.3307 - val_accuracy: 0.5360\n",
      "Epoch 9/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1687 - accuracy: 0.5747\n",
      "Epoch 9: val_accuracy improved from 0.56965 to 0.59628, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 146s 1s/step - loss: 1.1687 - accuracy: 0.5747 - val_loss: 1.0893 - val_accuracy: 0.5963\n",
      "Epoch 10/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1624 - accuracy: 0.5765\n",
      "Epoch 10: val_accuracy did not improve from 0.59628\n",
      "126/126 [==============================] - 145s 1s/step - loss: 1.1624 - accuracy: 0.5765 - val_loss: 1.1726 - val_accuracy: 0.5612\n",
      "Epoch 11/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1472 - accuracy: 0.5830\n",
      "Epoch 11: val_accuracy did not improve from 0.59628\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.1472 - accuracy: 0.5830 - val_loss: 1.1111 - val_accuracy: 0.5941\n",
      "Epoch 12/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1368 - accuracy: 0.5877\n",
      "Epoch 12: val_accuracy improved from 0.59628 to 0.60058, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 145s 1s/step - loss: 1.1368 - accuracy: 0.5877 - val_loss: 1.0928 - val_accuracy: 0.6006\n",
      "Epoch 13/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1302 - accuracy: 0.5904\n",
      "Epoch 13: val_accuracy did not improve from 0.60058\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.1302 - accuracy: 0.5904 - val_loss: 1.1024 - val_accuracy: 0.5914\n",
      "Epoch 14/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1220 - accuracy: 0.5957\n",
      "Epoch 14: val_accuracy did not improve from 0.60058\n",
      "126/126 [==============================] - 148s 1s/step - loss: 1.1220 - accuracy: 0.5957 - val_loss: 1.1416 - val_accuracy: 0.5751\n",
      "Epoch 15/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1131 - accuracy: 0.5997\n",
      "Epoch 15: val_accuracy did not improve from 0.60058\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.1131 - accuracy: 0.5997 - val_loss: 1.1085 - val_accuracy: 0.5981\n",
      "Epoch 16/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1037 - accuracy: 0.6020\n",
      "Epoch 16: val_accuracy improved from 0.60058 to 0.61219, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.1037 - accuracy: 0.6020 - val_loss: 1.0756 - val_accuracy: 0.6122\n",
      "Epoch 17/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.1012 - accuracy: 0.6026\n",
      "Epoch 17: val_accuracy improved from 0.61219 to 0.61465, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.1012 - accuracy: 0.6026 - val_loss: 1.0490 - val_accuracy: 0.6147\n",
      "Epoch 18/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0981 - accuracy: 0.6043\n",
      "Epoch 18: val_accuracy did not improve from 0.61465\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.0981 - accuracy: 0.6043 - val_loss: 1.1432 - val_accuracy: 0.5688\n",
      "Epoch 19/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.6078\n",
      "Epoch 19: val_accuracy did not improve from 0.61465\n",
      "126/126 [==============================] - 145s 1s/step - loss: 1.0886 - accuracy: 0.6078 - val_loss: 1.0708 - val_accuracy: 0.6071\n",
      "Epoch 20/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0855 - accuracy: 0.6085\n",
      "Epoch 20: val_accuracy did not improve from 0.61465\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.0855 - accuracy: 0.6085 - val_loss: 1.0763 - val_accuracy: 0.6038\n",
      "Epoch 21/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0805 - accuracy: 0.6107\n",
      "Epoch 21: val_accuracy did not improve from 0.61465\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0805 - accuracy: 0.6107 - val_loss: 1.0661 - val_accuracy: 0.6062\n",
      "Epoch 22/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0772 - accuracy: 0.6114\n",
      "Epoch 22: val_accuracy improved from 0.61465 to 0.61606, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.0772 - accuracy: 0.6114 - val_loss: 1.0532 - val_accuracy: 0.6161\n",
      "Epoch 23/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.6114\n",
      "Epoch 23: val_accuracy did not improve from 0.61606\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0744 - accuracy: 0.6114 - val_loss: 1.1530 - val_accuracy: 0.5808\n",
      "Epoch 24/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0695 - accuracy: 0.6144\n",
      "Epoch 24: val_accuracy did not improve from 0.61606\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0695 - accuracy: 0.6144 - val_loss: 1.0818 - val_accuracy: 0.6016\n",
      "Epoch 25/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0615 - accuracy: 0.6169\n",
      "Epoch 25: val_accuracy improved from 0.61606 to 0.62156, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.0615 - accuracy: 0.6169 - val_loss: 1.0451 - val_accuracy: 0.6216\n",
      "Epoch 26/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0638 - accuracy: 0.6166\n",
      "Epoch 26: val_accuracy improved from 0.62156 to 0.63023, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 1.0638 - accuracy: 0.6166 - val_loss: 1.0149 - val_accuracy: 0.6302\n",
      "Epoch 27/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0627 - accuracy: 0.6173\n",
      "Epoch 27: val_accuracy did not improve from 0.63023\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0627 - accuracy: 0.6173 - val_loss: 1.0135 - val_accuracy: 0.6288\n",
      "Epoch 28/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.6203\n",
      "Epoch 28: val_accuracy improved from 0.63023 to 0.63409, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 147s 1s/step - loss: 1.0543 - accuracy: 0.6203 - val_loss: 1.0004 - val_accuracy: 0.6341\n",
      "Epoch 29/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.6231\n",
      "Epoch 29: val_accuracy improved from 0.63409 to 0.63433, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0503 - accuracy: 0.6231 - val_loss: 1.0030 - val_accuracy: 0.6343\n",
      "Epoch 30/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.6205\n",
      "Epoch 30: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 146s 1s/step - loss: 1.0497 - accuracy: 0.6205 - val_loss: 1.0364 - val_accuracy: 0.6198\n",
      "Epoch 31/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0455 - accuracy: 0.6240\n",
      "Epoch 31: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 145s 1s/step - loss: 1.0455 - accuracy: 0.6240 - val_loss: 1.0146 - val_accuracy: 0.6258\n",
      "Epoch 32/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0411 - accuracy: 0.6251\n",
      "Epoch 32: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0411 - accuracy: 0.6251 - val_loss: 1.0084 - val_accuracy: 0.6306\n",
      "Epoch 33/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0377 - accuracy: 0.6273\n",
      "Epoch 33: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0377 - accuracy: 0.6273 - val_loss: 1.0037 - val_accuracy: 0.6338\n",
      "Epoch 34/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0414 - accuracy: 0.6265\n",
      "Epoch 34: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0414 - accuracy: 0.6265 - val_loss: 0.9951 - val_accuracy: 0.6324\n",
      "Epoch 35/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.6283\n",
      "Epoch 35: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0347 - accuracy: 0.6283 - val_loss: 1.0075 - val_accuracy: 0.6280\n",
      "Epoch 36/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.6296\n",
      "Epoch 36: val_accuracy did not improve from 0.63433\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0333 - accuracy: 0.6296 - val_loss: 1.0493 - val_accuracy: 0.6170\n",
      "Epoch 37/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0309 - accuracy: 0.6295\n",
      "Epoch 37: val_accuracy improved from 0.63433 to 0.64174, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0309 - accuracy: 0.6295 - val_loss: 0.9853 - val_accuracy: 0.6417\n",
      "Epoch 38/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0248 - accuracy: 0.6319\n",
      "Epoch 38: val_accuracy did not improve from 0.64174\n",
      "126/126 [==============================] - 148s 1s/step - loss: 1.0248 - accuracy: 0.6319 - val_loss: 0.9893 - val_accuracy: 0.6373\n",
      "Epoch 39/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0261 - accuracy: 0.6318\n",
      "Epoch 39: val_accuracy did not improve from 0.64174\n",
      "126/126 [==============================] - 147s 1s/step - loss: 1.0261 - accuracy: 0.6318 - val_loss: 0.9935 - val_accuracy: 0.6369\n",
      "Epoch 40/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0220 - accuracy: 0.6335\n",
      "Epoch 40: val_accuracy did not improve from 0.64174\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0220 - accuracy: 0.6335 - val_loss: 1.0537 - val_accuracy: 0.6144\n",
      "Epoch 41/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0182 - accuracy: 0.6344\n",
      "Epoch 41: val_accuracy improved from 0.64174 to 0.64739, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0182 - accuracy: 0.6344 - val_loss: 0.9729 - val_accuracy: 0.6474\n",
      "Epoch 42/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0169 - accuracy: 0.6363\n",
      "Epoch 42: val_accuracy did not improve from 0.64739\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0169 - accuracy: 0.6363 - val_loss: 0.9700 - val_accuracy: 0.6466\n",
      "Epoch 43/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0171 - accuracy: 0.6345\n",
      "Epoch 43: val_accuracy improved from 0.64739 to 0.64917, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0171 - accuracy: 0.6345 - val_loss: 0.9674 - val_accuracy: 0.6492\n",
      "Epoch 44/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0096 - accuracy: 0.6379\n",
      "Epoch 44: val_accuracy did not improve from 0.64917\n",
      "126/126 [==============================] - 146s 1s/step - loss: 1.0096 - accuracy: 0.6379 - val_loss: 1.0447 - val_accuracy: 0.6178\n",
      "Epoch 45/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0122 - accuracy: 0.6368\n",
      "Epoch 45: val_accuracy did not improve from 0.64917\n",
      "126/126 [==============================] - 148s 1s/step - loss: 1.0122 - accuracy: 0.6368 - val_loss: 0.9681 - val_accuracy: 0.6445\n",
      "Epoch 46/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0120 - accuracy: 0.6369\n",
      "Epoch 46: val_accuracy did not improve from 0.64917\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0120 - accuracy: 0.6369 - val_loss: 0.9724 - val_accuracy: 0.6416\n",
      "Epoch 47/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0094 - accuracy: 0.6379\n",
      "Epoch 47: val_accuracy did not improve from 0.64917\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0094 - accuracy: 0.6379 - val_loss: 0.9687 - val_accuracy: 0.6482\n",
      "Epoch 48/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0027 - accuracy: 0.6405\n",
      "Epoch 48: val_accuracy improved from 0.64917 to 0.64925, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0027 - accuracy: 0.6405 - val_loss: 0.9633 - val_accuracy: 0.6492\n",
      "Epoch 49/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0025 - accuracy: 0.6410\n",
      "Epoch 49: val_accuracy did not improve from 0.64925\n",
      "126/126 [==============================] - 142s 1s/step - loss: 1.0025 - accuracy: 0.6410 - val_loss: 0.9978 - val_accuracy: 0.6349\n",
      "Epoch 50/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0026 - accuracy: 0.6412\n",
      "Epoch 50: val_accuracy did not improve from 0.64925\n",
      "126/126 [==============================] - 143s 1s/step - loss: 1.0026 - accuracy: 0.6412 - val_loss: 0.9776 - val_accuracy: 0.6391\n",
      "Epoch 51/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9960 - accuracy: 0.6433\n",
      "Epoch 51: val_accuracy did not improve from 0.64925\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9960 - accuracy: 0.6433 - val_loss: 0.9701 - val_accuracy: 0.6422\n",
      "Epoch 52/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9964 - accuracy: 0.6434\n",
      "Epoch 52: val_accuracy improved from 0.64925 to 0.65161, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 0.9964 - accuracy: 0.6434 - val_loss: 0.9588 - val_accuracy: 0.6516\n",
      "Epoch 53/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9964 - accuracy: 0.6431\n",
      "Epoch 53: val_accuracy did not improve from 0.65161\n",
      "126/126 [==============================] - 144s 1s/step - loss: 0.9964 - accuracy: 0.6431 - val_loss: 1.0139 - val_accuracy: 0.6359\n",
      "Epoch 54/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9893 - accuracy: 0.6466\n",
      "Epoch 54: val_accuracy did not improve from 0.65161\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9893 - accuracy: 0.6466 - val_loss: 0.9963 - val_accuracy: 0.6384\n",
      "Epoch 55/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9901 - accuracy: 0.6457\n",
      "Epoch 55: val_accuracy improved from 0.65161 to 0.65820, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9901 - accuracy: 0.6457 - val_loss: 0.9377 - val_accuracy: 0.6582\n",
      "Epoch 56/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9895 - accuracy: 0.6453\n",
      "Epoch 56: val_accuracy did not improve from 0.65820\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9895 - accuracy: 0.6453 - val_loss: 0.9938 - val_accuracy: 0.6394\n",
      "Epoch 57/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9897 - accuracy: 0.6461\n",
      "Epoch 57: val_accuracy improved from 0.65820 to 0.66224, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 0.9897 - accuracy: 0.6461 - val_loss: 0.9343 - val_accuracy: 0.6622\n",
      "Epoch 58/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9858 - accuracy: 0.6451\n",
      "Epoch 58: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9858 - accuracy: 0.6451 - val_loss: 0.9906 - val_accuracy: 0.6360\n",
      "Epoch 59/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9853 - accuracy: 0.6485\n",
      "Epoch 59: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9853 - accuracy: 0.6485 - val_loss: 0.9604 - val_accuracy: 0.6544\n",
      "Epoch 60/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9811 - accuracy: 0.6498\n",
      "Epoch 60: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9811 - accuracy: 0.6498 - val_loss: 0.9787 - val_accuracy: 0.6451\n",
      "Epoch 61/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9857 - accuracy: 0.6473\n",
      "Epoch 61: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9857 - accuracy: 0.6473 - val_loss: 0.9883 - val_accuracy: 0.6341\n",
      "Epoch 62/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9810 - accuracy: 0.6493\n",
      "Epoch 62: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9810 - accuracy: 0.6493 - val_loss: 1.0098 - val_accuracy: 0.6359\n",
      "Epoch 63/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9823 - accuracy: 0.6480\n",
      "Epoch 63: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 144s 1s/step - loss: 0.9823 - accuracy: 0.6480 - val_loss: 0.9482 - val_accuracy: 0.6569\n",
      "Epoch 64/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9769 - accuracy: 0.6502\n",
      "Epoch 64: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9769 - accuracy: 0.6502 - val_loss: 0.9707 - val_accuracy: 0.6414\n",
      "Epoch 65/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.6503\n",
      "Epoch 65: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9773 - accuracy: 0.6503 - val_loss: 0.9571 - val_accuracy: 0.6539\n",
      "Epoch 66/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.6511\n",
      "Epoch 66: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9715 - accuracy: 0.6511 - val_loss: 0.9515 - val_accuracy: 0.6562\n",
      "Epoch 67/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.6523\n",
      "Epoch 67: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9715 - accuracy: 0.6523 - val_loss: 0.9463 - val_accuracy: 0.6520\n",
      "Epoch 68/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9708 - accuracy: 0.6542\n",
      "Epoch 68: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9708 - accuracy: 0.6542 - val_loss: 0.9359 - val_accuracy: 0.6562\n",
      "Epoch 69/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9670 - accuracy: 0.6532\n",
      "Epoch 69: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9670 - accuracy: 0.6532 - val_loss: 0.9698 - val_accuracy: 0.6444\n",
      "Epoch 70/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9648 - accuracy: 0.6548\n",
      "Epoch 70: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9648 - accuracy: 0.6548 - val_loss: 0.9828 - val_accuracy: 0.6454\n",
      "Epoch 71/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9663 - accuracy: 0.6562\n",
      "Epoch 71: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 143s 1s/step - loss: 0.9663 - accuracy: 0.6562 - val_loss: 0.9572 - val_accuracy: 0.6517\n",
      "Epoch 72/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9643 - accuracy: 0.6545\n",
      "Epoch 72: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9643 - accuracy: 0.6545 - val_loss: 0.9501 - val_accuracy: 0.6547\n",
      "Epoch 73/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9617 - accuracy: 0.6562\n",
      "Epoch 73: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9617 - accuracy: 0.6562 - val_loss: 0.9734 - val_accuracy: 0.6485\n",
      "Epoch 74/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9592 - accuracy: 0.6590\n",
      "Epoch 74: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9592 - accuracy: 0.6590 - val_loss: 0.9477 - val_accuracy: 0.6564\n",
      "Epoch 75/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9618 - accuracy: 0.6556\n",
      "Epoch 75: val_accuracy did not improve from 0.66224\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9618 - accuracy: 0.6556 - val_loss: 0.9510 - val_accuracy: 0.6540\n",
      "Epoch 76/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9571 - accuracy: 0.6569\n",
      "Epoch 76: val_accuracy improved from 0.66224 to 0.66320, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 144s 1s/step - loss: 0.9571 - accuracy: 0.6569 - val_loss: 0.9289 - val_accuracy: 0.6632\n",
      "Epoch 77/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9557 - accuracy: 0.6584\n",
      "Epoch 77: val_accuracy did not improve from 0.66320\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9557 - accuracy: 0.6584 - val_loss: 0.9877 - val_accuracy: 0.6411\n",
      "Epoch 78/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9604 - accuracy: 0.6573\n",
      "Epoch 78: val_accuracy did not improve from 0.66320\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9604 - accuracy: 0.6573 - val_loss: 0.9300 - val_accuracy: 0.6620\n",
      "Epoch 79/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9633 - accuracy: 0.6559\n",
      "Epoch 79: val_accuracy improved from 0.66320 to 0.66328, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9633 - accuracy: 0.6559 - val_loss: 0.9271 - val_accuracy: 0.6633\n",
      "Epoch 80/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.6598\n",
      "Epoch 80: val_accuracy did not improve from 0.66328\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9516 - accuracy: 0.6598 - val_loss: 0.9463 - val_accuracy: 0.6538\n",
      "Epoch 81/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9496 - accuracy: 0.6608\n",
      "Epoch 81: val_accuracy did not improve from 0.66328\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9496 - accuracy: 0.6608 - val_loss: 0.9527 - val_accuracy: 0.6536\n",
      "Epoch 82/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9471 - accuracy: 0.6612\n",
      "Epoch 82: val_accuracy did not improve from 0.66328\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9471 - accuracy: 0.6612 - val_loss: 0.9306 - val_accuracy: 0.6610\n",
      "Epoch 83/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 0.6613\n",
      "Epoch 83: val_accuracy did not improve from 0.66328\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9472 - accuracy: 0.6613 - val_loss: 0.9364 - val_accuracy: 0.6628\n",
      "Epoch 84/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9451 - accuracy: 0.6618\n",
      "Epoch 84: val_accuracy improved from 0.66328 to 0.66629, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9451 - accuracy: 0.6618 - val_loss: 0.9184 - val_accuracy: 0.6663\n",
      "Epoch 85/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9470 - accuracy: 0.6616\n",
      "Epoch 85: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9470 - accuracy: 0.6616 - val_loss: 0.9624 - val_accuracy: 0.6531\n",
      "Epoch 86/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9485 - accuracy: 0.6605\n",
      "Epoch 86: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9485 - accuracy: 0.6605 - val_loss: 0.9383 - val_accuracy: 0.6592\n",
      "Epoch 87/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9423 - accuracy: 0.6635\n",
      "Epoch 87: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9423 - accuracy: 0.6635 - val_loss: 0.9443 - val_accuracy: 0.6563\n",
      "Epoch 88/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9403 - accuracy: 0.6643\n",
      "Epoch 88: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9403 - accuracy: 0.6643 - val_loss: 0.9386 - val_accuracy: 0.6610\n",
      "Epoch 89/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9388 - accuracy: 0.6645\n",
      "Epoch 89: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9388 - accuracy: 0.6645 - val_loss: 0.9571 - val_accuracy: 0.6497\n",
      "Epoch 90/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9400 - accuracy: 0.6636\n",
      "Epoch 90: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9400 - accuracy: 0.6636 - val_loss: 0.9776 - val_accuracy: 0.6459\n",
      "Epoch 91/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9393 - accuracy: 0.6654\n",
      "Epoch 91: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9393 - accuracy: 0.6654 - val_loss: 0.9381 - val_accuracy: 0.6589\n",
      "Epoch 92/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9401 - accuracy: 0.6632\n",
      "Epoch 92: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9401 - accuracy: 0.6632 - val_loss: 0.9441 - val_accuracy: 0.6580\n",
      "Epoch 93/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9330 - accuracy: 0.6660\n",
      "Epoch 93: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9330 - accuracy: 0.6660 - val_loss: 0.9396 - val_accuracy: 0.6588\n",
      "Epoch 94/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9340 - accuracy: 0.6668\n",
      "Epoch 94: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9340 - accuracy: 0.6668 - val_loss: 0.9312 - val_accuracy: 0.6622\n",
      "Epoch 95/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.6675\n",
      "Epoch 95: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9297 - accuracy: 0.6675 - val_loss: 0.9358 - val_accuracy: 0.6596\n",
      "Epoch 96/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9287 - accuracy: 0.6684\n",
      "Epoch 96: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9287 - accuracy: 0.6684 - val_loss: 0.9274 - val_accuracy: 0.6634\n",
      "Epoch 97/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9281 - accuracy: 0.6680\n",
      "Epoch 97: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9281 - accuracy: 0.6680 - val_loss: 0.9648 - val_accuracy: 0.6511\n",
      "Epoch 98/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.6696\n",
      "Epoch 98: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9258 - accuracy: 0.6696 - val_loss: 0.9250 - val_accuracy: 0.6655\n",
      "Epoch 99/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.6700\n",
      "Epoch 99: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9258 - accuracy: 0.6700 - val_loss: 0.9256 - val_accuracy: 0.6623\n",
      "Epoch 100/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9230 - accuracy: 0.6707\n",
      "Epoch 100: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9230 - accuracy: 0.6707 - val_loss: 0.9297 - val_accuracy: 0.6635\n",
      "Epoch 101/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9261 - accuracy: 0.6708\n",
      "Epoch 101: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9261 - accuracy: 0.6708 - val_loss: 0.9637 - val_accuracy: 0.6525\n",
      "Epoch 102/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9205 - accuracy: 0.6702\n",
      "Epoch 102: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9205 - accuracy: 0.6702 - val_loss: 0.9491 - val_accuracy: 0.6571\n",
      "Epoch 103/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9217 - accuracy: 0.6716\n",
      "Epoch 103: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9217 - accuracy: 0.6716 - val_loss: 1.0138 - val_accuracy: 0.6312\n",
      "Epoch 104/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.6721\n",
      "Epoch 104: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9226 - accuracy: 0.6721 - val_loss: 0.9842 - val_accuracy: 0.6461\n",
      "Epoch 105/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9198 - accuracy: 0.6714\n",
      "Epoch 105: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9198 - accuracy: 0.6714 - val_loss: 0.9627 - val_accuracy: 0.6508\n",
      "Epoch 106/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9179 - accuracy: 0.6719\n",
      "Epoch 106: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9179 - accuracy: 0.6719 - val_loss: 0.9397 - val_accuracy: 0.6568\n",
      "Epoch 107/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6747\n",
      "Epoch 107: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9135 - accuracy: 0.6747 - val_loss: 1.0148 - val_accuracy: 0.6330\n",
      "Epoch 108/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6710\n",
      "Epoch 108: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9174 - accuracy: 0.6710 - val_loss: 0.9723 - val_accuracy: 0.6486\n",
      "Epoch 109/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6746\n",
      "Epoch 109: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 142s 1s/step - loss: 0.9151 - accuracy: 0.6746 - val_loss: 0.9490 - val_accuracy: 0.6557\n",
      "Epoch 110/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6732\n",
      "Epoch 110: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9138 - accuracy: 0.6732 - val_loss: 0.9390 - val_accuracy: 0.6580\n",
      "Epoch 111/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9155 - accuracy: 0.6727\n",
      "Epoch 111: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9155 - accuracy: 0.6727 - val_loss: 0.9470 - val_accuracy: 0.6571\n",
      "Epoch 112/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.6752\n",
      "Epoch 112: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9111 - accuracy: 0.6752 - val_loss: 0.9415 - val_accuracy: 0.6605\n",
      "Epoch 113/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6769\n",
      "Epoch 113: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9066 - accuracy: 0.6769 - val_loss: 0.9368 - val_accuracy: 0.6611\n",
      "Epoch 114/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.6776\n",
      "Epoch 114: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9049 - accuracy: 0.6776 - val_loss: 0.9691 - val_accuracy: 0.6467\n",
      "Epoch 115/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9074 - accuracy: 0.6759\n",
      "Epoch 115: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9074 - accuracy: 0.6759 - val_loss: 0.9633 - val_accuracy: 0.6523\n",
      "Epoch 116/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6779\n",
      "Epoch 116: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9034 - accuracy: 0.6779 - val_loss: 0.9498 - val_accuracy: 0.6585\n",
      "Epoch 117/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.6806\n",
      "Epoch 117: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9010 - accuracy: 0.6806 - val_loss: 0.9484 - val_accuracy: 0.6568\n",
      "Epoch 118/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9014 - accuracy: 0.6787\n",
      "Epoch 118: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9014 - accuracy: 0.6787 - val_loss: 0.9530 - val_accuracy: 0.6511\n",
      "Epoch 119/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9016 - accuracy: 0.6779\n",
      "Epoch 119: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9016 - accuracy: 0.6779 - val_loss: 0.9618 - val_accuracy: 0.6552\n",
      "Epoch 120/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9016 - accuracy: 0.6775\n",
      "Epoch 120: val_accuracy did not improve from 0.66629\n",
      "126/126 [==============================] - 141s 1s/step - loss: 0.9016 - accuracy: 0.6775 - val_loss: 0.9933 - val_accuracy: 0.6390\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "checkpoint_path = 'checkpoints/check.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=True,\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "his = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=120,\n",
    "    callbacks=[tensorboard_callback, checkpoint]\n",
    ")\n",
    "\n",
    "plot_loss(his, save=True)\n",
    "\n",
    "# This will send the a notification on supported systems\n",
    "# !notify-send -i \"$(pwd)/fig.png\" \"AI: finished training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa1a0199e40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('checkpoints/check.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = image_dataset_from_directory(\n",
    "  './test', \n",
    "  labels=None, \n",
    "  shuffle=False,\n",
    "  color_mode=color_mode,\n",
    "  batch_size=1,\n",
    "  image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 0, 0, 5, 7, 6, 0, 0, 6]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [np.argmax(x) for x in model.predict(test_ds)]\n",
    "# Only print a limited amount of predictions\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame(data={'Cell type': predictions}).to_csv('predictions.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 380k/380k [00:01<00:00, 258kB/s]\n",
      "Successfully submitted to COM2028 21/22 CW"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -m $URN -c uos-com2028-21-22-cw -f predictions.csv"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
