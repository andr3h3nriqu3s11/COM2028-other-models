{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, utils\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, to_categorical\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = ''  # Your Kaggle username\n",
    "os.environ['KAGGLE_KEY'] = ''  # Your Kaggle API key\n",
    "os.environ['URN'] = '6644818'  # Your URN: submissions without a URN will not count#\n",
    "\n",
    "# !kaggle competitions download -c uos-com2028-21-22-cw()\n",
    "# !unzip uos-com2028-21-22-cw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 09:15:31.075804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.129502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.129648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mode = 'grayscale'\n",
    "image_size = (32, 32)\n",
    "image_shape = (*image_size, 1)\n",
    "batch_size = 800\n",
    "\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 09:15:31.243753: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-11 09:15:31.244654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.244899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.245075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.936446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.936596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.936762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 09:15:31.937117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5446 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# read all labels\n",
    "train_labels_dp = pd.read_csv('train.csv')\n",
    "train_labels = tf.constant(train_labels_dp.loc[:, 'Cell type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "def pathToLabel(path):\n",
    "  path = tf.strings.regex_replace(path, \"./train/\", \"\")\n",
    "  path = tf.strings.regex_replace(path, \".jpg\", \"\")\n",
    "  return train_labels[tf.strings.to_number(path, out_type=tf.int32)]\n",
    "\n",
    "def decode_image(img):\n",
    "  # channels were reduced to 1 since image is grayscale\n",
    "  img = tf.io.decode_jpeg(img, channels=1)\n",
    "\n",
    "  return tf.image.resize(img, image_size)\n",
    "\n",
    "def process_path(path, addPath=False):\n",
    "  label = pathToLabel(path)\n",
    "\n",
    "  img = tf.io.read_file(path)\n",
    "  img = decode_image(img)\n",
    "  img = tf.image.adjust_gamma(img, 1.1)\n",
    "  if addPath:\n",
    "    return img, label, path\n",
    "  else:\n",
    "    return img, label\n",
    "\n",
    "def process_path_test(path, addPath=False):\n",
    "  img = tf.io.read_file(path)\n",
    "  img = decode_image(img)\n",
    "  img = tf.image.adjust_gamma(img, 1.1)\n",
    "  if addPath:\n",
    "    return img, path\n",
    "  else:\n",
    "    return img\n",
    "\n",
    "def configure_for_performance(ds: tf.data.Dataset) -> tf.data.Dataset:\n",
    "  #ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size= 1000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def prepare_dataset(ds: tf.data.Dataset) -> tf.data.Dataset:\n",
    "  ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "  ds = configure_for_performance(ds)\n",
    "  return ds\n",
    "\n",
    "def prepare_dataset_test(ds: tf.data.Dataset) -> tf.data.Dataset:\n",
    "  ds = ds.map(process_path_test, num_parallel_calls=AUTOTUNE)\n",
    "  ds = ds.batch(1)\n",
    "  ds = ds.prefetch(AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 8825\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 10000)\n",
    "\n",
    "print(\"seed: %d\" % seed)\n",
    "\n",
    "# Read all the files from the direcotry\n",
    "list_ds = tf.data.Dataset.list_files(str('./train/*'), shuffle=False)\n",
    "\n",
    "image_count = len(list_ds)\n",
    "\n",
    "list_ds = list_ds.shuffle(image_count, seed=seed)\n",
    "\n",
    "val_size = int(image_count * 0.333)\n",
    "\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n",
    "\n",
    "train_ds = prepare_dataset(train_ds)\n",
    "val_ds = prepare_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images\n",
    "# image_batch, label_batch  = next(iter(train_ds))\n",
    "\n",
    "# plt.figure(figsize=image_size)\n",
    "# for i in range(4):\n",
    "#   ax = plt.subplot(2, 2, i + 1)\n",
    "#   plt.imshow(image_batch[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "\n",
    "#   label = label_batch[i]\n",
    "#   path = \"\"#str(path_batch[i].numpy())\n",
    "\n",
    "#   plt.title(str(label.numpy()) + \" \" + path)\n",
    "#   plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_6 (Sequential)   (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " rescaling_3 (Rescaling)     (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " 0-1-64-1 (Sequential)       (None, 32, 32, 64)        384       \n",
      "                                                                 \n",
      " 1-2-128-3 (Sequential)      (None, 16, 16, 128)       221952    \n",
      "                                                                 \n",
      " 2-2-250-3 (Sequential)      (None, 8, 8, 250)         852000    \n",
      "                                                                 \n",
      " 3-2-500-3 (Sequential)      (None, 4, 4, 500)         3378000   \n",
      "                                                                 \n",
      " 4-1-1000-3 (Sequential)     (None, 4, 4, 1000)        4505000   \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 1000)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 8008      \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,965,344\n",
      "Trainable params: 8,961,460\n",
      "Non-trainable params: 3,884\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=image_shape),\n",
    "    layers.RandomRotation(0.4),\n",
    "    layers.RandomContrast(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "track = 0\n",
    "\n",
    "\n",
    "def addBlock(\n",
    "    b_size: int,\n",
    "    filter_size: int,\n",
    "    kernel_size: int = 3,\n",
    "    top: bool = True,\n",
    "    pooling_same: bool = False,\n",
    "    pool_func=layers.MaxPool2D\n",
    "):\n",
    "    global track\n",
    "    model = keras.Sequential(name=f\"{track}-{b_size}-{filter_size}-{kernel_size}\")\n",
    "    track += 1\n",
    "    for _ in range(b_size):\n",
    "        model.add(layers.Conv2D(\n",
    "            filter_size,\n",
    "            kernel_size,\n",
    "            padding=\"same\"\n",
    "        ))\n",
    "        model.add(layers.ReLU())\n",
    "    if top:\n",
    "        if pooling_same:\n",
    "            model.add(pool_func(padding=\"same\", strides=(1, 1)))\n",
    "        else:\n",
    "            model.add(pool_func())\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(data_augmentation)\n",
    "\n",
    "model.add(layers.Rescaling(1.0/255, input_shape=image_shape))\n",
    "\n",
    "model.add(addBlock(1, 64, 1, pooling_same=True, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(2, 128, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(2, 250, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(2, 500, 3, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(addBlock(1, 1000, 3, pooling_same=True, pool_func=layers.AveragePooling2D))\n",
    "\n",
    "model.add(layers.GlobalAvgPool2D())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Softmax())\n",
    "\n",
    "model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  optimizer=tfa.optimizers.AdamW(weight_decay=1e-4),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/keras/regression\n",
    "def plot_loss(history, val=True, color=\"b\", save=False):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    if val:\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    if save:\n",
    "        plt.savefig('./fig.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.2265 - accuracy: 0.5480"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 13:38:47.386813: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1080000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24501, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 101s 802ms/step - loss: 1.2264 - accuracy: 0.5480 - val_loss: 3.2134 - val_accuracy: 0.2450\n",
      "Epoch 2/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.1778 - accuracy: 0.5641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 13:40:27.419029: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1080000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.24501 to 0.37329, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 100s 793ms/step - loss: 1.1779 - accuracy: 0.5641 - val_loss: 1.7094 - val_accuracy: 0.3733\n",
      "Epoch 3/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.1478 - accuracy: 0.5753"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 13:42:09.949600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1080000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_accuracy improved from 0.37329 to 0.45854, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 102s 813ms/step - loss: 1.1477 - accuracy: 0.5753 - val_loss: 1.4543 - val_accuracy: 0.4585\n",
      "Epoch 4/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.1233 - accuracy: 0.5845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 13:43:53.620369: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1080000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_accuracy improved from 0.45854 to 0.56919, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 104s 822ms/step - loss: 1.1233 - accuracy: 0.5845 - val_loss: 1.1359 - val_accuracy: 0.5692\n",
      "Epoch 5/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.1057 - accuracy: 0.5902\n",
      "Epoch 5: val_accuracy improved from 0.56919 to 0.59942, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 105s 837ms/step - loss: 1.1057 - accuracy: 0.5902 - val_loss: 1.0703 - val_accuracy: 0.5994\n",
      "Epoch 6/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0907 - accuracy: 0.5958\n",
      "Epoch 6: val_accuracy did not improve from 0.59942\n",
      "126/126 [==============================] - 106s 841ms/step - loss: 1.0907 - accuracy: 0.5958 - val_loss: 1.0964 - val_accuracy: 0.5953\n",
      "Epoch 7/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.6013\n",
      "Epoch 7: val_accuracy did not improve from 0.59942\n",
      "126/126 [==============================] - 103s 815ms/step - loss: 1.0795 - accuracy: 0.6013 - val_loss: 1.0895 - val_accuracy: 0.5975\n",
      "Epoch 8/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0738 - accuracy: 0.6025\n",
      "Epoch 8: val_accuracy improved from 0.59942 to 0.62090, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 102s 809ms/step - loss: 1.0737 - accuracy: 0.6025 - val_loss: 1.0385 - val_accuracy: 0.6209\n",
      "Epoch 9/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0636 - accuracy: 0.6076\n",
      "Epoch 9: val_accuracy improved from 0.62090 to 0.62551, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 109s 861ms/step - loss: 1.0636 - accuracy: 0.6076 - val_loss: 1.0209 - val_accuracy: 0.6255\n",
      "Epoch 10/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0547 - accuracy: 0.6097\n",
      "Epoch 10: val_accuracy improved from 0.62551 to 0.62567, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 109s 866ms/step - loss: 1.0547 - accuracy: 0.6097 - val_loss: 1.0120 - val_accuracy: 0.6257\n",
      "Epoch 11/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0508 - accuracy: 0.6114\n",
      "Epoch 11: val_accuracy did not improve from 0.62567\n",
      "126/126 [==============================] - 106s 838ms/step - loss: 1.0509 - accuracy: 0.6113 - val_loss: 1.0800 - val_accuracy: 0.6026\n",
      "Epoch 12/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0413 - accuracy: 0.6160\n",
      "Epoch 12: val_accuracy did not improve from 0.62567\n",
      "126/126 [==============================] - 105s 831ms/step - loss: 1.0412 - accuracy: 0.6160 - val_loss: 1.0372 - val_accuracy: 0.6160\n",
      "Epoch 13/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0395 - accuracy: 0.6162\n",
      "Epoch 13: val_accuracy did not improve from 0.62567\n",
      "126/126 [==============================] - 103s 818ms/step - loss: 1.0396 - accuracy: 0.6162 - val_loss: 1.0362 - val_accuracy: 0.6190\n",
      "Epoch 14/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.6176\n",
      "Epoch 14: val_accuracy improved from 0.62567 to 0.62789, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 105s 831ms/step - loss: 1.0329 - accuracy: 0.6176 - val_loss: 0.9972 - val_accuracy: 0.6279\n",
      "Epoch 15/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.6190\n",
      "Epoch 15: val_accuracy did not improve from 0.62789\n",
      "126/126 [==============================] - 107s 849ms/step - loss: 1.0293 - accuracy: 0.6190 - val_loss: 1.0527 - val_accuracy: 0.6125\n",
      "Epoch 16/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.6221\n",
      "Epoch 16: val_accuracy did not improve from 0.62789\n",
      "126/126 [==============================] - 107s 847ms/step - loss: 1.0223 - accuracy: 0.6221 - val_loss: 1.0330 - val_accuracy: 0.6228\n",
      "Epoch 17/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0161 - accuracy: 0.6251\n",
      "Epoch 17: val_accuracy did not improve from 0.62789\n",
      "126/126 [==============================] - 108s 855ms/step - loss: 1.0161 - accuracy: 0.6251 - val_loss: 1.0026 - val_accuracy: 0.6242\n",
      "Epoch 18/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.6249\n",
      "Epoch 18: val_accuracy did not improve from 0.62789\n",
      "126/126 [==============================] - 105s 837ms/step - loss: 1.0162 - accuracy: 0.6249 - val_loss: 1.0562 - val_accuracy: 0.6004\n",
      "Epoch 19/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.6277\n",
      "Epoch 19: val_accuracy did not improve from 0.62789\n",
      "126/126 [==============================] - 108s 853ms/step - loss: 1.0103 - accuracy: 0.6276 - val_loss: 1.0415 - val_accuracy: 0.6101\n",
      "Epoch 20/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0110 - accuracy: 0.6260\n",
      "Epoch 20: val_accuracy improved from 0.62789 to 0.64653, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 110s 870ms/step - loss: 1.0111 - accuracy: 0.6260 - val_loss: 0.9654 - val_accuracy: 0.6465\n",
      "Epoch 21/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 1.0073 - accuracy: 0.6283\n",
      "Epoch 21: val_accuracy did not improve from 0.64653\n",
      "126/126 [==============================] - 107s 847ms/step - loss: 1.0073 - accuracy: 0.6283 - val_loss: 0.9564 - val_accuracy: 0.6443\n",
      "Epoch 22/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9981 - accuracy: 0.6320\n",
      "Epoch 22: val_accuracy did not improve from 0.64653\n",
      "126/126 [==============================] - 110s 869ms/step - loss: 0.9980 - accuracy: 0.6320 - val_loss: 1.0002 - val_accuracy: 0.6337\n",
      "Epoch 23/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9957 - accuracy: 0.6324\n",
      "Epoch 23: val_accuracy did not improve from 0.64653\n",
      "126/126 [==============================] - 105s 834ms/step - loss: 0.9957 - accuracy: 0.6325 - val_loss: 0.9751 - val_accuracy: 0.6406\n",
      "Epoch 24/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9949 - accuracy: 0.6327\n",
      "Epoch 24: val_accuracy did not improve from 0.64653\n",
      "126/126 [==============================] - 105s 834ms/step - loss: 0.9949 - accuracy: 0.6326 - val_loss: 0.9629 - val_accuracy: 0.6432\n",
      "Epoch 25/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9931 - accuracy: 0.6350\n",
      "Epoch 25: val_accuracy improved from 0.64653 to 0.65203, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 105s 835ms/step - loss: 0.9930 - accuracy: 0.6350 - val_loss: 0.9439 - val_accuracy: 0.6520\n",
      "Epoch 26/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9848 - accuracy: 0.6368\n",
      "Epoch 26: val_accuracy did not improve from 0.65203\n",
      "126/126 [==============================] - 107s 847ms/step - loss: 0.9849 - accuracy: 0.6368 - val_loss: 0.9559 - val_accuracy: 0.6435\n",
      "Epoch 27/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9897 - accuracy: 0.6343\n",
      "Epoch 27: val_accuracy did not improve from 0.65203\n",
      "126/126 [==============================] - 104s 823ms/step - loss: 0.9897 - accuracy: 0.6343 - val_loss: 0.9480 - val_accuracy: 0.6499\n",
      "Epoch 28/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9878 - accuracy: 0.6359\n",
      "Epoch 28: val_accuracy did not improve from 0.65203\n",
      "126/126 [==============================] - 103s 819ms/step - loss: 0.9880 - accuracy: 0.6359 - val_loss: 0.9644 - val_accuracy: 0.6428\n",
      "Epoch 29/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9841 - accuracy: 0.6377\n",
      "Epoch 29: val_accuracy improved from 0.65203 to 0.65401, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 104s 828ms/step - loss: 0.9842 - accuracy: 0.6377 - val_loss: 0.9376 - val_accuracy: 0.6540\n",
      "Epoch 30/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9833 - accuracy: 0.6357\n",
      "Epoch 30: val_accuracy improved from 0.65401 to 0.65520, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 100s 796ms/step - loss: 0.9833 - accuracy: 0.6357 - val_loss: 0.9451 - val_accuracy: 0.6552\n",
      "Epoch 31/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9733 - accuracy: 0.6433\n",
      "Epoch 31: val_accuracy did not improve from 0.65520\n",
      "126/126 [==============================] - 100s 790ms/step - loss: 0.9734 - accuracy: 0.6433 - val_loss: 0.9283 - val_accuracy: 0.6545\n",
      "Epoch 32/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9763 - accuracy: 0.6385\n",
      "Epoch 32: val_accuracy did not improve from 0.65520\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.9763 - accuracy: 0.6385 - val_loss: 0.9845 - val_accuracy: 0.6363\n",
      "Epoch 33/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9675 - accuracy: 0.6417\n",
      "Epoch 33: val_accuracy did not improve from 0.65520\n",
      "126/126 [==============================] - 100s 789ms/step - loss: 0.9675 - accuracy: 0.6417 - val_loss: 0.9512 - val_accuracy: 0.6478\n",
      "Epoch 34/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9711 - accuracy: 0.6416\n",
      "Epoch 34: val_accuracy improved from 0.65520 to 0.65600, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 100s 795ms/step - loss: 0.9710 - accuracy: 0.6416 - val_loss: 0.9272 - val_accuracy: 0.6560\n",
      "Epoch 35/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9674 - accuracy: 0.6442\n",
      "Epoch 35: val_accuracy did not improve from 0.65600\n",
      "126/126 [==============================] - 109s 864ms/step - loss: 0.9674 - accuracy: 0.6441 - val_loss: 0.9579 - val_accuracy: 0.6420\n",
      "Epoch 36/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9645 - accuracy: 0.6439\n",
      "Epoch 36: val_accuracy did not improve from 0.65600\n",
      "126/126 [==============================] - 100s 792ms/step - loss: 0.9646 - accuracy: 0.6438 - val_loss: 0.9367 - val_accuracy: 0.6549\n",
      "Epoch 37/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9672 - accuracy: 0.6428\n",
      "Epoch 37: val_accuracy did not improve from 0.65600\n",
      "126/126 [==============================] - 100s 790ms/step - loss: 0.9672 - accuracy: 0.6428 - val_loss: 0.9501 - val_accuracy: 0.6534\n",
      "Epoch 38/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9637 - accuracy: 0.6450\n",
      "Epoch 38: val_accuracy improved from 0.65600 to 0.66358, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 100s 789ms/step - loss: 0.9637 - accuracy: 0.6450 - val_loss: 0.9153 - val_accuracy: 0.6636\n",
      "Epoch 39/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9620 - accuracy: 0.6443\n",
      "Epoch 39: val_accuracy did not improve from 0.66358\n",
      "126/126 [==============================] - 100s 797ms/step - loss: 0.9620 - accuracy: 0.6443 - val_loss: 0.9585 - val_accuracy: 0.6435\n",
      "Epoch 40/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.6469\n",
      "Epoch 40: val_accuracy did not improve from 0.66358\n",
      "126/126 [==============================] - 103s 818ms/step - loss: 0.9575 - accuracy: 0.6469 - val_loss: 0.9541 - val_accuracy: 0.6509\n",
      "Epoch 41/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9602 - accuracy: 0.6459\n",
      "Epoch 41: val_accuracy did not improve from 0.66358\n",
      "126/126 [==============================] - 104s 822ms/step - loss: 0.9602 - accuracy: 0.6460 - val_loss: 0.9184 - val_accuracy: 0.6598\n",
      "Epoch 42/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9588 - accuracy: 0.6465\n",
      "Epoch 42: val_accuracy did not improve from 0.66358\n",
      "126/126 [==============================] - 106s 843ms/step - loss: 0.9589 - accuracy: 0.6465 - val_loss: 0.9336 - val_accuracy: 0.6574\n",
      "Epoch 43/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9508 - accuracy: 0.6479\n",
      "Epoch 43: val_accuracy improved from 0.66358 to 0.66631, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 103s 814ms/step - loss: 0.9508 - accuracy: 0.6479 - val_loss: 0.9031 - val_accuracy: 0.6663\n",
      "Epoch 44/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9554 - accuracy: 0.6482\n",
      "Epoch 44: val_accuracy did not improve from 0.66631\n",
      "126/126 [==============================] - 105s 831ms/step - loss: 0.9554 - accuracy: 0.6482 - val_loss: 0.9602 - val_accuracy: 0.6476\n",
      "Epoch 45/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9512 - accuracy: 0.6477\n",
      "Epoch 45: val_accuracy did not improve from 0.66631\n",
      "126/126 [==============================] - 104s 826ms/step - loss: 0.9512 - accuracy: 0.6476 - val_loss: 0.9501 - val_accuracy: 0.6468\n",
      "Epoch 46/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9488 - accuracy: 0.6500\n",
      "Epoch 46: val_accuracy did not improve from 0.66631\n",
      "126/126 [==============================] - 109s 860ms/step - loss: 0.9487 - accuracy: 0.6500 - val_loss: 0.9083 - val_accuracy: 0.6637\n",
      "Epoch 47/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9471 - accuracy: 0.6504\n",
      "Epoch 47: val_accuracy did not improve from 0.66631\n",
      "126/126 [==============================] - 105s 833ms/step - loss: 0.9471 - accuracy: 0.6504 - val_loss: 0.9139 - val_accuracy: 0.6636\n",
      "Epoch 48/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9453 - accuracy: 0.6502\n",
      "Epoch 48: val_accuracy did not improve from 0.66631\n",
      "126/126 [==============================] - 107s 849ms/step - loss: 0.9453 - accuracy: 0.6502 - val_loss: 0.9299 - val_accuracy: 0.6587\n",
      "Epoch 49/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9405 - accuracy: 0.6527\n",
      "Epoch 49: val_accuracy improved from 0.66631 to 0.67379, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 103s 821ms/step - loss: 0.9405 - accuracy: 0.6527 - val_loss: 0.8924 - val_accuracy: 0.6738\n",
      "Epoch 50/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9411 - accuracy: 0.6535\n",
      "Epoch 50: val_accuracy did not improve from 0.67379\n",
      "126/126 [==============================] - 100s 792ms/step - loss: 0.9412 - accuracy: 0.6535 - val_loss: 0.8962 - val_accuracy: 0.6688\n",
      "Epoch 51/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9461 - accuracy: 0.6506\n",
      "Epoch 51: val_accuracy did not improve from 0.67379\n",
      "126/126 [==============================] - 100s 790ms/step - loss: 0.9462 - accuracy: 0.6506 - val_loss: 0.9242 - val_accuracy: 0.6534\n",
      "Epoch 52/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9438 - accuracy: 0.6533\n",
      "Epoch 52: val_accuracy did not improve from 0.67379\n",
      "126/126 [==============================] - 100s 794ms/step - loss: 0.9437 - accuracy: 0.6534 - val_loss: 0.9006 - val_accuracy: 0.6684\n",
      "Epoch 53/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9379 - accuracy: 0.6545\n",
      "Epoch 53: val_accuracy improved from 0.67379 to 0.67686, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 101s 801ms/step - loss: 0.9380 - accuracy: 0.6546 - val_loss: 0.8827 - val_accuracy: 0.6769\n",
      "Epoch 54/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9384 - accuracy: 0.6530\n",
      "Epoch 54: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.9384 - accuracy: 0.6529 - val_loss: 0.9028 - val_accuracy: 0.6679\n",
      "Epoch 55/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9368 - accuracy: 0.6541\n",
      "Epoch 55: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 101s 803ms/step - loss: 0.9369 - accuracy: 0.6541 - val_loss: 0.9088 - val_accuracy: 0.6649\n",
      "Epoch 56/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9344 - accuracy: 0.6561\n",
      "Epoch 56: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 101s 800ms/step - loss: 0.9345 - accuracy: 0.6561 - val_loss: 0.8936 - val_accuracy: 0.6692\n",
      "Epoch 57/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9371 - accuracy: 0.6529\n",
      "Epoch 57: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 106s 837ms/step - loss: 0.9372 - accuracy: 0.6528 - val_loss: 0.9209 - val_accuracy: 0.6586\n",
      "Epoch 58/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.6536\n",
      "Epoch 58: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 101s 801ms/step - loss: 0.9377 - accuracy: 0.6536 - val_loss: 0.9205 - val_accuracy: 0.6579\n",
      "Epoch 59/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9357 - accuracy: 0.6549\n",
      "Epoch 59: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 100s 794ms/step - loss: 0.9357 - accuracy: 0.6550 - val_loss: 0.8983 - val_accuracy: 0.6698\n",
      "Epoch 60/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9328 - accuracy: 0.6557\n",
      "Epoch 60: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 100s 790ms/step - loss: 0.9328 - accuracy: 0.6556 - val_loss: 0.8892 - val_accuracy: 0.6717\n",
      "Epoch 61/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6574\n",
      "Epoch 61: val_accuracy did not improve from 0.67686\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.9277 - accuracy: 0.6574 - val_loss: 0.8932 - val_accuracy: 0.6675\n",
      "Epoch 62/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.6573\n",
      "Epoch 62: val_accuracy improved from 0.67686 to 0.67746, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 99s 787ms/step - loss: 0.9306 - accuracy: 0.6572 - val_loss: 0.8794 - val_accuracy: 0.6775\n",
      "Epoch 63/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9296 - accuracy: 0.6574\n",
      "Epoch 63: val_accuracy did not improve from 0.67746\n",
      "126/126 [==============================] - 101s 805ms/step - loss: 0.9296 - accuracy: 0.6575 - val_loss: 0.8768 - val_accuracy: 0.6755\n",
      "Epoch 64/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.6576\n",
      "Epoch 64: val_accuracy did not improve from 0.67746\n",
      "126/126 [==============================] - 103s 814ms/step - loss: 0.9280 - accuracy: 0.6576 - val_loss: 0.8835 - val_accuracy: 0.6720\n",
      "Epoch 65/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9281 - accuracy: 0.6582\n",
      "Epoch 65: val_accuracy did not improve from 0.67746\n",
      "126/126 [==============================] - 102s 810ms/step - loss: 0.9282 - accuracy: 0.6582 - val_loss: 0.9088 - val_accuracy: 0.6642\n",
      "Epoch 66/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9251 - accuracy: 0.6587\n",
      "Epoch 66: val_accuracy did not improve from 0.67746\n",
      "126/126 [==============================] - 100s 793ms/step - loss: 0.9254 - accuracy: 0.6586 - val_loss: 0.8979 - val_accuracy: 0.6702\n",
      "Epoch 67/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9287 - accuracy: 0.6571\n",
      "Epoch 67: val_accuracy improved from 0.67746 to 0.67884, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 104s 825ms/step - loss: 0.9287 - accuracy: 0.6571 - val_loss: 0.8696 - val_accuracy: 0.6788\n",
      "Epoch 68/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9274 - accuracy: 0.6572\n",
      "Epoch 68: val_accuracy did not improve from 0.67884\n",
      "126/126 [==============================] - 101s 803ms/step - loss: 0.9274 - accuracy: 0.6572 - val_loss: 0.8931 - val_accuracy: 0.6712\n",
      "Epoch 69/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6612\n",
      "Epoch 69: val_accuracy improved from 0.67884 to 0.68036, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 101s 801ms/step - loss: 0.9203 - accuracy: 0.6612 - val_loss: 0.8690 - val_accuracy: 0.6804\n",
      "Epoch 70/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6610\n",
      "Epoch 70: val_accuracy did not improve from 0.68036\n",
      "126/126 [==============================] - 101s 799ms/step - loss: 0.9215 - accuracy: 0.6610 - val_loss: 0.8901 - val_accuracy: 0.6700\n",
      "Epoch 71/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.6601\n",
      "Epoch 71: val_accuracy did not improve from 0.68036\n",
      "126/126 [==============================] - 112s 889ms/step - loss: 0.9212 - accuracy: 0.6600 - val_loss: 0.8771 - val_accuracy: 0.6751\n",
      "Epoch 72/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9208 - accuracy: 0.6615\n",
      "Epoch 72: val_accuracy did not improve from 0.68036\n",
      "126/126 [==============================] - 114s 903ms/step - loss: 0.9208 - accuracy: 0.6614 - val_loss: 0.9003 - val_accuracy: 0.6693\n",
      "Epoch 73/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6603\n",
      "Epoch 73: val_accuracy did not improve from 0.68036\n",
      "126/126 [==============================] - 118s 934ms/step - loss: 0.9193 - accuracy: 0.6603 - val_loss: 0.8810 - val_accuracy: 0.6754\n",
      "Epoch 74/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9167 - accuracy: 0.6612\n",
      "Epoch 74: val_accuracy improved from 0.68036 to 0.68382, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 114s 906ms/step - loss: 0.9167 - accuracy: 0.6612 - val_loss: 0.8649 - val_accuracy: 0.6838\n",
      "Epoch 75/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9154 - accuracy: 0.6642\n",
      "Epoch 75: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 113s 899ms/step - loss: 0.9155 - accuracy: 0.6642 - val_loss: 0.9091 - val_accuracy: 0.6630\n",
      "Epoch 76/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6622\n",
      "Epoch 76: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 113s 894ms/step - loss: 0.9170 - accuracy: 0.6623 - val_loss: 0.8784 - val_accuracy: 0.6737\n",
      "Epoch 77/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.6632\n",
      "Epoch 77: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 111s 883ms/step - loss: 0.9120 - accuracy: 0.6632 - val_loss: 0.8956 - val_accuracy: 0.6696\n",
      "Epoch 78/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.6655\n",
      "Epoch 78: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 112s 890ms/step - loss: 0.9122 - accuracy: 0.6655 - val_loss: 0.9100 - val_accuracy: 0.6637\n",
      "Epoch 79/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9109 - accuracy: 0.6646\n",
      "Epoch 79: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 103s 820ms/step - loss: 0.9109 - accuracy: 0.6647 - val_loss: 0.8879 - val_accuracy: 0.6704\n",
      "Epoch 80/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9109 - accuracy: 0.6644\n",
      "Epoch 80: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 104s 826ms/step - loss: 0.9108 - accuracy: 0.6644 - val_loss: 0.8637 - val_accuracy: 0.6813\n",
      "Epoch 81/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9100 - accuracy: 0.6657\n",
      "Epoch 81: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 102s 805ms/step - loss: 0.9099 - accuracy: 0.6657 - val_loss: 0.8744 - val_accuracy: 0.6770\n",
      "Epoch 82/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9139 - accuracy: 0.6621\n",
      "Epoch 82: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 107s 847ms/step - loss: 0.9139 - accuracy: 0.6621 - val_loss: 0.8705 - val_accuracy: 0.6764\n",
      "Epoch 83/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6651\n",
      "Epoch 83: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 102s 805ms/step - loss: 0.9062 - accuracy: 0.6651 - val_loss: 0.8833 - val_accuracy: 0.6736\n",
      "Epoch 84/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9079 - accuracy: 0.6676\n",
      "Epoch 84: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 102s 807ms/step - loss: 0.9079 - accuracy: 0.6676 - val_loss: 0.8946 - val_accuracy: 0.6702\n",
      "Epoch 85/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9091 - accuracy: 0.6651\n",
      "Epoch 85: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 117s 928ms/step - loss: 0.9091 - accuracy: 0.6651 - val_loss: 0.8705 - val_accuracy: 0.6797\n",
      "Epoch 86/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9054 - accuracy: 0.6660\n",
      "Epoch 86: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 105s 833ms/step - loss: 0.9054 - accuracy: 0.6660 - val_loss: 0.8750 - val_accuracy: 0.6780\n",
      "Epoch 87/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9058 - accuracy: 0.6676\n",
      "Epoch 87: val_accuracy did not improve from 0.68382\n",
      "126/126 [==============================] - 103s 813ms/step - loss: 0.9059 - accuracy: 0.6676 - val_loss: 0.8691 - val_accuracy: 0.6774\n",
      "Epoch 88/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9069 - accuracy: 0.6651\n",
      "Epoch 88: val_accuracy improved from 0.68382 to 0.68845, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 102s 808ms/step - loss: 0.9069 - accuracy: 0.6651 - val_loss: 0.8541 - val_accuracy: 0.6884\n",
      "Epoch 89/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9061 - accuracy: 0.6655\n",
      "Epoch 89: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 101s 798ms/step - loss: 0.9060 - accuracy: 0.6655 - val_loss: 0.8847 - val_accuracy: 0.6692\n",
      "Epoch 90/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9059 - accuracy: 0.6641\n",
      "Epoch 90: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 101s 802ms/step - loss: 0.9058 - accuracy: 0.6641 - val_loss: 0.8597 - val_accuracy: 0.6844\n",
      "Epoch 91/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9066 - accuracy: 0.6649\n",
      "Epoch 91: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.9065 - accuracy: 0.6649 - val_loss: 0.8609 - val_accuracy: 0.6845\n",
      "Epoch 92/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9067 - accuracy: 0.6640\n",
      "Epoch 92: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.9067 - accuracy: 0.6640 - val_loss: 0.8641 - val_accuracy: 0.6816\n",
      "Epoch 93/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9002 - accuracy: 0.6678\n",
      "Epoch 93: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 100s 790ms/step - loss: 0.9002 - accuracy: 0.6678 - val_loss: 0.8614 - val_accuracy: 0.6800\n",
      "Epoch 94/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9053 - accuracy: 0.6666\n",
      "Epoch 94: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 105s 833ms/step - loss: 0.9054 - accuracy: 0.6666 - val_loss: 0.8632 - val_accuracy: 0.6844\n",
      "Epoch 95/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9029 - accuracy: 0.6683\n",
      "Epoch 95: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 103s 815ms/step - loss: 0.9029 - accuracy: 0.6683 - val_loss: 0.8727 - val_accuracy: 0.6790\n",
      "Epoch 96/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8994 - accuracy: 0.6676\n",
      "Epoch 96: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 102s 812ms/step - loss: 0.8995 - accuracy: 0.6676 - val_loss: 0.8759 - val_accuracy: 0.6764\n",
      "Epoch 97/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8998 - accuracy: 0.6686\n",
      "Epoch 97: val_accuracy did not improve from 0.68845\n",
      "126/126 [==============================] - 107s 847ms/step - loss: 0.8998 - accuracy: 0.6686 - val_loss: 0.8598 - val_accuracy: 0.6860\n",
      "Epoch 98/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8962 - accuracy: 0.6712\n",
      "Epoch 98: val_accuracy improved from 0.68845 to 0.69007, saving model to checkpoints/check.ckpt\n",
      "126/126 [==============================] - 106s 839ms/step - loss: 0.8961 - accuracy: 0.6712 - val_loss: 0.8471 - val_accuracy: 0.6901\n",
      "Epoch 99/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8951 - accuracy: 0.6707\n",
      "Epoch 99: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 106s 842ms/step - loss: 0.8949 - accuracy: 0.6708 - val_loss: 0.8611 - val_accuracy: 0.6832\n",
      "Epoch 100/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8972 - accuracy: 0.6682\n",
      "Epoch 100: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 107s 851ms/step - loss: 0.8971 - accuracy: 0.6682 - val_loss: 0.8648 - val_accuracy: 0.6821\n",
      "Epoch 101/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.9001 - accuracy: 0.6694\n",
      "Epoch 101: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 106s 843ms/step - loss: 0.9000 - accuracy: 0.6695 - val_loss: 0.8745 - val_accuracy: 0.6775\n",
      "Epoch 102/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.8937 - accuracy: 0.6702\n",
      "Epoch 102: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 107s 848ms/step - loss: 0.8937 - accuracy: 0.6702 - val_loss: 0.8538 - val_accuracy: 0.6859\n",
      "Epoch 103/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8961 - accuracy: 0.6705\n",
      "Epoch 103: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 107s 846ms/step - loss: 0.8961 - accuracy: 0.6705 - val_loss: 0.8589 - val_accuracy: 0.6837\n",
      "Epoch 104/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8888 - accuracy: 0.6702\n",
      "Epoch 104: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 107s 849ms/step - loss: 0.8888 - accuracy: 0.6702 - val_loss: 0.8425 - val_accuracy: 0.6881\n",
      "Epoch 105/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8981 - accuracy: 0.6691\n",
      "Epoch 105: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 105s 835ms/step - loss: 0.8981 - accuracy: 0.6691 - val_loss: 0.8582 - val_accuracy: 0.6863\n",
      "Epoch 106/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8956 - accuracy: 0.6690\n",
      "Epoch 106: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 106s 835ms/step - loss: 0.8956 - accuracy: 0.6691 - val_loss: 0.8747 - val_accuracy: 0.6837\n",
      "Epoch 107/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8905 - accuracy: 0.6721\n",
      "Epoch 107: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 103s 821ms/step - loss: 0.8904 - accuracy: 0.6722 - val_loss: 0.8715 - val_accuracy: 0.6796\n",
      "Epoch 108/120\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.8923 - accuracy: 0.6723\n",
      "Epoch 108: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 103s 816ms/step - loss: 0.8923 - accuracy: 0.6723 - val_loss: 0.8684 - val_accuracy: 0.6810\n",
      "Epoch 109/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8919 - accuracy: 0.6726\n",
      "Epoch 109: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 106s 839ms/step - loss: 0.8919 - accuracy: 0.6725 - val_loss: 0.8520 - val_accuracy: 0.6877\n",
      "Epoch 110/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8921 - accuracy: 0.6715\n",
      "Epoch 110: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 104s 826ms/step - loss: 0.8921 - accuracy: 0.6715 - val_loss: 0.8761 - val_accuracy: 0.6764\n",
      "Epoch 111/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8924 - accuracy: 0.6722\n",
      "Epoch 111: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 99s 788ms/step - loss: 0.8923 - accuracy: 0.6723 - val_loss: 0.8555 - val_accuracy: 0.6830\n",
      "Epoch 112/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8878 - accuracy: 0.6741\n",
      "Epoch 112: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 99s 788ms/step - loss: 0.8879 - accuracy: 0.6741 - val_loss: 0.8774 - val_accuracy: 0.6743\n",
      "Epoch 113/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8921 - accuracy: 0.6714\n",
      "Epoch 113: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 100s 795ms/step - loss: 0.8921 - accuracy: 0.6713 - val_loss: 0.8887 - val_accuracy: 0.6673\n",
      "Epoch 114/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8915 - accuracy: 0.6709\n",
      "Epoch 114: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 100s 792ms/step - loss: 0.8915 - accuracy: 0.6708 - val_loss: 0.8755 - val_accuracy: 0.6777\n",
      "Epoch 115/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8912 - accuracy: 0.6726\n",
      "Epoch 115: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 102s 812ms/step - loss: 0.8913 - accuracy: 0.6725 - val_loss: 0.8444 - val_accuracy: 0.6880\n",
      "Epoch 116/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8884 - accuracy: 0.6717\n",
      "Epoch 116: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 102s 809ms/step - loss: 0.8884 - accuracy: 0.6717 - val_loss: 0.8671 - val_accuracy: 0.6819\n",
      "Epoch 117/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8889 - accuracy: 0.6729\n",
      "Epoch 117: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 108s 855ms/step - loss: 0.8889 - accuracy: 0.6729 - val_loss: 0.8591 - val_accuracy: 0.6836\n",
      "Epoch 118/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8852 - accuracy: 0.6738\n",
      "Epoch 118: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 104s 824ms/step - loss: 0.8853 - accuracy: 0.6737 - val_loss: 0.8648 - val_accuracy: 0.6816\n",
      "Epoch 119/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8877 - accuracy: 0.6739\n",
      "Epoch 119: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 103s 819ms/step - loss: 0.8877 - accuracy: 0.6739 - val_loss: 0.8656 - val_accuracy: 0.6803\n",
      "Epoch 120/120\n",
      "125/126 [============================>.] - ETA: 0s - loss: 0.8856 - accuracy: 0.6737\n",
      "Epoch 120: val_accuracy did not improve from 0.69007\n",
      "126/126 [==============================] - 100s 791ms/step - loss: 0.8856 - accuracy: 0.6737 - val_loss: 0.8462 - val_accuracy: 0.6890\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "checkpoint_path = 'checkpoints/check.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=True,\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "his = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=120,\n",
    "    callbacks=[tensorboard_callback, checkpoint]\n",
    ")\n",
    "\n",
    "plot_loss(his, save=True)\n",
    "\n",
    "!notify-send -i \"$(pwd)/fig.png\" \"AI: finished training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test_now.h5')\n",
    "# model = keras.models.load_model('test_now.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe1483e2e60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('checkpoints/check.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = image_dataset_from_directory(\n",
    "  './test', \n",
    "  labels=None, \n",
    "  shuffle=False,\n",
    "  color_mode=color_mode,\n",
    "  batch_size=1,\n",
    "  image_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_ds = tf.data.Dataset.list_files(str('./test/*'), shuffle=False)\n",
    "\n",
    "test_ds = prepare_dataset_test(test_list_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 7, 0, 4, 6, 6, 0, 7, 6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [np.argmax(x) for x in model.predict(test_ds)]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame(data={'Cell type': predictions}).to_csv('predictions.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 380k/380k [00:01<00:00, 255kB/s]\n",
      "Successfully submitted to COM2028 21/22 CW"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -m $URN -c uos-com2028-21-22-cw -f predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
